{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Importando as bibliotecas nescessarias."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# -*- coding: utf-8 -*-\r\n",
    "from flask import Flask\r\n",
    "from flask import jsonify\r\n",
    "from spacy import load\r\n",
    "from scrapy.spiders import XMLFeedSpider\r\n",
    "from scrapy.crawler import CrawlerProcess\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import xml.etree.ElementTree as ET\r\n",
    "import json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iniciando variaveis globais."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#python -m spacy download pt_core_news_lg\r\n",
    "nlp = load('pt_core_news_lg')\r\n",
    "noticias = []\r\n",
    "noti = 5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Criando função para reestruturar os dados para o formato JSON."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def reestruturar(noticias):\r\n",
    "    resposta = []\r\n",
    "    for i in range(len(noticias)):\r\n",
    "        resposta.append({\r\n",
    "            \"titulo\": str(noticias[i][\"titulo\"]),\r\n",
    "            \"link\": str(noticias[i][\"link\"]),\r\n",
    "            \"conteudo\": str(noticias[i][\"conteudo\"]),\r\n",
    "            \"entidades\": noticias[i][\"entidades\"]\r\n",
    "        })\r\n",
    "    return resposta"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Criando o Crawler para fazer a extração dos dados do site financenews."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ExtrairNoticias(XMLFeedSpider):\r\n",
    "    name = 'extrairnoticias'\r\n",
    "    allowed_domains = ['financenews.com.br']\r\n",
    "    start_urls = ['https://financenews.com.br/feed/']\r\n",
    "    iterator = 'xml'\r\n",
    "    itertag = 'rss'\r\n",
    "\r\n",
    "    def parse_node(self, response, selector):\r\n",
    "        response.selector.register_namespace(\r\n",
    "            'content', 'http://purl.org/rss/1.0/modules/content/')\r\n",
    "        tree = ET.ElementTree(ET.fromstring(selector.extract()))\r\n",
    "        root = tree.getroot()\r\n",
    "        ns = {'content': 'http://purl.org/rss/1.0/modules/content/'}\r\n",
    "        conteudos = []\r\n",
    "        for chanel in root.findall(\"channel\"):\r\n",
    "            for child in chanel.findall(\"item\"):\r\n",
    "                for title in child.findall(\"content:encoded\", ns):\r\n",
    "                    tasdf = BeautifulSoup(\r\n",
    "                        title.text, 'html.parser').find_all('p')\r\n",
    "                    conteudo = ''\r\n",
    "                    for h in tasdf:\r\n",
    "                        if h.find_all('span') != []:\r\n",
    "                            conteudo = str(conteudo) + \\\r\n",
    "                                str(h.find('span').text)+\"  \"\r\n",
    "                conteudos.append(conteudo)\r\n",
    "        canais = selector.xpath('/rss/channel/item/title/text()').extract()\r\n",
    "        links = selector.xpath('/rss/channel/item/link/text()').extract()\r\n",
    "        cont = 0\r\n",
    "        for i in range(len(selector.xpath('/rss/channel/item').extract())):\r\n",
    "            if cont == 5:\r\n",
    "                break\r\n",
    "            if str(conteudos[i].replace(u'\\xa0', u' ')) != '':\r\n",
    "                noticias.append({\"titulo\": str(canais[i].replace(\r\n",
    "                    u'\\xa0', u' ')), \"link\": str(links[i]), \"conteudo\": str(conteudos[i].replace(u'\\xa0', u' ')), \"entidades\": []})\r\n",
    "                cont += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inicinado a extração de noticias do site financenews."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "process = CrawlerProcess({\r\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\r\n",
    "})\r\n",
    "process.crawl(ExtrairNoticias)\r\n",
    "process.start()\r\n",
    "process.stop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Salvando dados no JSON noticias.json."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('noticias.json', 'w', encoding='utf8') as json_file:\r\n",
    "    json.dump(reestruturar(noticias), json_file, ensure_ascii=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ler lendo as noticias salvas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('noticias.json', 'r', encoding='utf8') as f:\r\n",
    "    noticias = json.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenizando e extraindo as entidades dos conteudos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for noticia in noticias:\r\n",
    "    for entidade in nlp(noticia['conteudo']).ents:\r\n",
    "        noticia['entidades'].append({\"entidade\": str(entidade.text), \"tipo\": str(entidade.label_)})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Salvando os dados em um arquivo JSON."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('dados.json', 'w', encoding='utf8') as json_file:\r\n",
    "    json.dump(reestruturar(noticias), json_file, ensure_ascii=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mostrando todos os dados coletados no Flask."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "app = Flask(__name__)\r\n",
    "app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True\r\n",
    "app.config['JSON_AS_ASCII'] = False\r\n",
    "@app.route(\"/\")\r\n",
    "def hello():\r\n",
    "    return jsonify(reestruturar(noticias))\r\n",
    "app.run(host=\"127.0.0.1\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "a8f1f614f4a9c18380787ee873145773021b0eb912acee23b46ed3c1ad2de9d9"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}